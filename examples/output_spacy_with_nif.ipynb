{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ae67ac",
   "metadata": {},
   "source": [
    "# From Spacy to NIF-compliant RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9321c1d4",
   "metadata": {},
   "source": [
    "First of all, if you are running on your computer you have some setting up to do! Uncomment and run the following cells if you need to install the appropriate libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "51f43dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy.cli\n",
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c49cd",
   "metadata": {},
   "source": [
    "Anyway, we **all** need to install my [spacy2nif](https://github.com/francescomambrini/Spacy2NIF) module to convert the `spacy` annotation into NIF/RDF. Run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b83a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/francescomambrini/Spacy2NIF.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954ec27",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372b993",
   "metadata": {},
   "source": [
    "See the [documentation](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86284aa",
   "metadata": {},
   "source": [
    "`spaCy` \"is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. [...] Unlike `NLTK`, which is widely used for teaching and research, spaCy focuses on providing software for production usage. `spaCy` also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc. Using Thinc as its backend, `spaCy` features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and named entity recognition (NER). Prebuilt statistical neural network models to perform these tasks are available for 23 languages, including English, Portuguese, Spanish, Russian and Chinese, and there is also a multi-language NER model. Additional support for tokenization for more than 65 languages allows users to train custom models on their own datasets as well\" (from [Wikipedia](https://en.wikipedia.org/wiki/SpaCy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689aa77",
   "metadata": {},
   "source": [
    "Here is how we import the library and load the model. For most languages, models come in different versions, including in particular:\n",
    "- lightweight models (faster, smaller but less accurate), generally identified with the `sm` tag.\n",
    "- bigger models that are heavier for computation and bigger (might not be supported by all computers); identified with `md`, `lg` and `trf` indicator.\n",
    "\n",
    "For this experiment we are just happy with the most basic model for English: `en_core_web_sm`. The name means:\n",
    " \n",
    "- `core`: general-purpose pipeline\n",
    "- `web`: genre of training data for the pipeline (web content)\n",
    "- `sm`: the size (small model)\n",
    "\n",
    "See [here](https://spacy.io/models) for instructions on how to download and load different models for other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef55ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''Three times Randolph Carter dreamed of the marvellous city, and three times was he snatched away while still he paused on the high terrace above it. All golden and lovely it blazed in the sunset, with walls, temples, colonnades, and arched bridges of veined marble, silver-basined fountains of prismatic spray in broad squares and perfumed gardens, and wide streets marching between delicate trees and blossom-laden urns and ivory statues in gleaming rows; while on steep northward slopes climbed tiers of red roofs and old peaked gables harbouring little lanes of grassy cobbles. It was a fever of the gods; a fanfare of supernal trumpets and a clash of immortal cymbals. Mystery hung about it as clouds about a fabulous unvisited mountain; and as Carter stood breathless and expectant on that balustraded parapet there swept up to him the poignancy and suspense of almost-vanished memory, the pain of lost things, and the maddening need to place again what once had an awesome and momentous place.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f4581",
   "metadata": {},
   "source": [
    "In the following code cell we run the pipeline on the text that we defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a95ca",
   "metadata": {},
   "source": [
    "The pipeline processed our document. But what did it do, exactly? Here is a summary of the pipeline components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b862b5c",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "If you loop over a `doc`, or if you index the doc with a number between 0 (first token) and $n$ (nr. of tokens in a text - 1), you access the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0], type(doc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eae531",
   "metadata": {},
   "source": [
    "Tokens hold a series o properties stored in attribute, including their string form (their text) and other annotations produced by the pipeline, like POS tags, lemmatization, morphological analysis and other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc[:5]:\n",
    "    print(t.text, t.pos_, t.lemma_, t.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24bb2d",
   "metadata": {},
   "source": [
    "But even more crucially for our purposes, the tokens store the offsets of the character where the token starts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7104ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc[:5]:\n",
    "    print(t.text, t.idx, t.idx + len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99f41f",
   "metadata": {},
   "source": [
    "### Sentence splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a897d",
   "metadata": {},
   "source": [
    "Since we have sentence splitting in our pipeline, let's inspect the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc.sents:\n",
    "    print(f\"{e.text}\\t{e.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4c6a5",
   "metadata": {},
   "source": [
    "We can inspect one of those sentences even closer. Sentences are generated also to support dependency parsing, which is explicitely marked as a component of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ae9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = next(doc.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a774e3",
   "metadata": {},
   "source": [
    "The visualizer [`displacy`](https://spacy.io/usage/visualizers/) allows us to graphically inspect the parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sent, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640eaac",
   "metadata": {},
   "source": [
    "But the dependency relation is hardcoded also in the tokens' attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how I access the first token here?\n",
    "t = doc[0]\n",
    "print (f\"{t.head} -[{t.dep_}]-> {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f4d61",
   "metadata": {},
   "source": [
    "I can get the root th sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sent.root\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e521097",
   "metadata": {},
   "source": [
    "I can obtain the list of any token's dependents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `r` is the root of the sentence\n",
    "for c in r.children:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cdc8f6",
   "metadata": {},
   "source": [
    "## Named Entity\n",
    "\n",
    "NER is also in our pipeline, so let's inspect how it went. The list of the annotated entities is accessible from the `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc.ents:\n",
    "    print(e.text, e.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6379842",
   "metadata": {},
   "source": [
    "You see that an entity can be a token, but it can also be something else..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3752338",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc.ents:\n",
    "    print(e.text, type(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1372abb1",
   "metadata": {},
   "source": [
    "In fact, entities are correctly indicated as spans (btw, sentences are span as well), which may or may not correspond to single tokens (in the case of \"Randolph Cater\" for instance it does not).\n",
    "\n",
    "Spans also register the start and end offset, which comes very handy to generate NIF-comliant representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc.ents:\n",
    "    print(e.text, e.start_char, e.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68706d65",
   "metadata": {},
   "source": [
    "The same thing can be done with sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaef240",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "    print(f\"{s.text[:5]}...\", s.start_char, s.end_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf31bce",
   "metadata": {},
   "source": [
    "Once again, we can use `displaicy` to have a look at the entity annotation within the document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f919b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc086a",
   "metadata": {},
   "source": [
    "## Convert to RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf6968",
   "metadata": {},
   "source": [
    "Let us load and check the converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a1285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import spacy2nif.exporter\n",
    "\n",
    "importlib.reload(spacy2nif.exporter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9af00",
   "metadata": {},
   "source": [
    "Here we initialize the converter. You can pass a `base_uri` argument to it. If you don't do it, it defaults to http://example.org/doc#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nif = spacy2nif.exporter.NIFExporter(base_uri=\"http://example.org/doc#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nif.export_doc(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ef1e3",
   "metadata": {},
   "source": [
    "Let's bind the namespaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.bind('nif', str(nif.NIF))\n",
    "g.bind('conll', str(nif.CONLL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cc2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize('doc2nif.ttl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3cedf",
   "metadata": {},
   "source": [
    "## The Shadow Over Insmouth\n",
    "\n",
    "Let's do an exercise. We are going to take the [horror novella](https://en.wikipedia.org/wiki/The_Shadow_over_Innsmouth) *The Shadow Over Insmouth* by H.P. Lovecraft and we are going to:\n",
    "\n",
    "1. annotate it\n",
    "2. export the annotation as NIF\n",
    "\n",
    "We will get the text of the novella from the [lovecraftcorpus](https://github.com/vilmibm/lovecraftcorpus) on GitHub. The URL of the txt is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7750c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/vilmibm/lovecraftcorpus/refs/heads/master/innsmouth.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e6043f",
   "metadata": {},
   "source": [
    "Let's retrieve the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(url)\n",
    "txt = r.text\n",
    "print(txt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8405e516",
   "metadata": {},
   "source": [
    "Now we annotate it using the same pipeline as before (it may take a while, depending on your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644978c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eebea76",
   "metadata": {},
   "source": [
    "Now let's convert it to NIF. We use the web URL of the raw text in GitHub as the document base URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "nif = spacy2nif.exporter.NIFExporter(base_uri=f\"{url}#\", export_full_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615011f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nif.export_doc(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59dea1",
   "metadata": {},
   "source": [
    "Let's bind the namespaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f996e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.bind('nif', str(nif.NIF))\n",
    "g.bind('conll', str(nif.CONLL))\n",
    "g.bind('insmouth', f\"{url}#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02da88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize('insmouth.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9968fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(next(doc.sents), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454208e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15942014",
   "metadata": {},
   "source": [
    "## Appendix: IOB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cdc1f7",
   "metadata": {},
   "source": [
    "Most NER applications use a common format known as [IOB](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)) as output. IOB annotation looks like this:\n",
    "\n",
    "```\n",
    "During\tO\n",
    "the\tB-DATE\n",
    "winter\tI-DATE\n",
    "of\tI-DATE\n",
    "1927\tI-DATE\n",
    "officials\tO\n",
    "of\tO\n",
    "the\tO\n",
    "Federal\tB-ORG\n",
    "government\tI-ORG\n",
    "in\n",
    "Boston\tB-LOC\n",
    "...\n",
    "```\n",
    "\n",
    "As you may have guessed, the tags like `I-ORG` or `B-LOC` are composed of two parts: 1. the tag (`ORG`, `LOC` etc.) or an `O` if the token is *not* a Named Entity; 2. a prefix that is used for chunking.\n",
    "\n",
    "Prefixes are:\n",
    "\n",
    "- `B`: for the token that either start the span of the NE, or is the only token of the NE\n",
    "- `I`: for the token that either is in the middle or is the last in the span \n",
    "\n",
    "(there are many flavours, however! Implementations may change, especially in how they use `I` and `B` for spans made of only one token or for final tokens. The explanation give above works for the output of [NameTag](https://lindat.mff.cuni.cz/services/nametag/) that is quoted above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e12c9",
   "metadata": {},
   "source": [
    "## Appendix 2: Entity Linking in `spacy`\n",
    "\n",
    "`spacy` provides functionalities to perform entity linking, i.e. to take the named entities and match it with IDs from Knowledge Bases.\n",
    "\n",
    "There are couple of solutions for doing this. One is included in `spacy` and it is pluggable as a component in the pipline. It requires to create your custom knowledge base.\n",
    "\n",
    "The other one is the [spacy-entity-linker](https://pypi.org/project/spacy-entity-linker/) module by Martino Mensio. It must be installed and, the first time it is used, it will download the Wikidata as a spacy KB to be used (~1.3 giga!!!). The good new, if you're using Colab, is that it will be downloaded on your VM (but be aware that the **free tier of Colab has limited space**, of around 100gb)...\n",
    "\n",
    "But actually, I wasn't very sastified with that.\n",
    "\n",
    "We can try a thirt option! We can:\n",
    "\n",
    "1. build a CSV file with all the URIs of the NE\n",
    "2. load it into OpenRefine\n",
    "3. use the reconciliation with Wikidata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba19eae",
   "metadata": {},
   "source": [
    "**IMPORTANT**\n",
    "\n",
    "Make sure you have whatever text file you want uploaded on your Colab machine (right pane, click on the folder and then upload)!\n",
    "\n",
    "If you want to follow what I did in the video you can upload [this](https://github.com/francescomambrini/Spacy2NIF/blob/main/examples/louvre-ticket-price-hike-scli-intl.txt) txt file with a [news article](https://lite.cnn.com/2025/11/28/travel/louvre-ticket-price-hike-scli-intl#) from CNN.\n",
    "\n",
    "Once you have loaded it, here is how you open it, read it and process it with `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "182248aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# or use whatever model you want\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('louvre-ticket-price-hike-scli-intl.txt') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8cf8a",
   "metadata": {},
   "source": [
    "And here is how we generate the CSV file to be read by OpenRefine. If you are working with your own data, make sure to update the `base_uri` according to your settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d0bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_uri = \"https://lite.cnn.com/2025/11/28/travel/louvre-ticket-price-hike-scli-intl#\"\n",
    "\n",
    "with open('louvre.csv', 'w') as out:\n",
    "  out.write('URI\\tText\\tLabel\\n')\n",
    "  for e in doc.ents:\n",
    "    uri = f'{base_uri}char={e.start_char},{e.end_char}'\n",
    "    out.write(f'{uri}\\t{e.text.replace('\\n', ' ')}\\t{e.label_}\\n')\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
